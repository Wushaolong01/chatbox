import os
import json
import logging
import tempfile
import gradio as gr
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_community.embeddings import FastEmbedEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.langchain import LangchainEmbedding
from llama_index.core.utils import truncate_text
from PIL import Image
import pytesseract
from langchain_ollama import ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from unstructured.partition.auto import partition
from unstructured.staging.base import convert_to_isd
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List

# ç¯å¢ƒé…ç½®
os.environ["TOKENIZERS_PARALLELISM"] = "false"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ¨¡å‹é…ç½®
MODEL_CONFIG = {
    "embedding": {
        "name": "BAAI/bge-small-en-v1.5",
        "cache_dir": "./models/embeddings"
    },
    "llm": {
        "name": "llama2",
        "temperature": 0.7,
        "num_ctx": 4096
    }
}

SUPPORTED_EXTS = [
    ".pdf", ".txt", ".docx", ".doc",
    ".xlsx", ".pptx", ".ppt",
    ".png", ".jpg", ".jpeg"
]

# ä¿®æ”¹åçš„æš—è‰²ç³»ä¸»é¢˜é…ç½®
custom_theme = gr.themes.Default(
    primary_hue="pink",
    secondary_hue="purple",
    neutral_hue="slate",
    radius_size="md",
    font=[gr.themes.GoogleFont("Inter"), "ui-sans-serif", "system-ui"]
).set(
    button_primary_background_fill="linear-gradient(90deg, #FF69B4 0%, #FF1493 100%)",
    button_primary_text_color="#ffffff",
    button_primary_background_fill_hover="linear-gradient(90deg, #FF1493 0%, #FF6EB4 100%)",
    button_primary_border_color="#FF69B4",
    block_background_fill="#1A1A1A",
    block_label_background_fill="#2D2D2D",
    block_label_text_color="#FFB6C1",
    block_title_text_color="#FF69B4",
    body_background_fill="#121212",
    border_color_accent="#FF69B4",
    color_accent_soft="#4A1E35",
    input_background_fill="#2D2D2D",
    input_border_color="#4A1E35",
    shadow_spread="6px",
    slider_color="#FF69B4"
)

class MarkdownConverter:
    @staticmethod
    def convert_to_markdown(file_path: str) -> str:
        """å°†æ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        try:
            elements = partition(filename=file_path, strategy="fast")
            return "\n\n".join(e.text for e in elements if hasattr(e, "text"))
        except Exception as e:
            logger.error(f"Markdownè½¬æ¢å¤±è´¥: {str(e)}")
            raise


class EmbeddingManager:
    @classmethod
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=10))
    def get_embedder(cls):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        os.makedirs(MODEL_CONFIG["embedding"]["cache_dir"], exist_ok=True)
        return FastEmbedEmbeddings(
            model_name=MODEL_CONFIG["embedding"]["name"],
            cache_dir=MODEL_CONFIG["embedding"]["cache_dir"],
            threads=4
        )


class ImageProcessor:
    @staticmethod
    def extract_text_from_image(image_path, lang='chi_sim+eng'):
        """OCRæ–‡æœ¬æå–"""
        try:
            img = Image.open(image_path)
            return pytesseract.image_to_string(img, config=f'--oem 3 --psm 6 -l {lang}').strip()
        except Exception as e:
            logger.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}")
            raise


class DataProcessor:
    @staticmethod
    def _create_temp_file(content: str, suffix: str) -> str:
        """åˆ›å»ºä¸´æ—¶æ–‡ä»¶"""
        with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=suffix) as tmp:
            tmp.write(content)
            return tmp.name

    @classmethod
    def _process_single_file(cls, file_path: str):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            if file_path.lower().endswith((".png", ".jpg", ".jpeg")):
                content = f"# å›¾ç‰‡å†…å®¹\n\n{ImageProcessor.extract_text_from_image(file_path)}"
            else:
                content = MarkdownConverter.convert_to_markdown(file_path)

            tmp_path = cls._create_temp_file(content, ".md")
            loader = TextLoader(tmp_path)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=800,
                chunk_overlap=150,
                separators=[
                    "\n\n# ", "\n## ", "\n### ", "\n\n", "\n"
                ]
            )
            pages = loader.load_and_split(text_splitter=text_splitter)

            for doc in pages:
                doc.metadata.update({
                    "source": file_path,
                    "file_type": os.path.splitext(file_path)[1][1:],
                    "converted": True
                })

            os.remove(tmp_path)
            return filter_complex_metadata(pages)
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {str(e)}")
            raise

    @classmethod
    def process_files(cls, file_paths: List[str]) -> Chroma:
        """å¹¶è¡Œå¤„ç†æ–‡ä»¶"""
        all_docs = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(cls._process_single_file, fp): fp for fp in file_paths}
            for future in as_completed(futures):
                try:
                    all_docs.extend(future.result())
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {futures[future]}: {str(e)}")
        return Chroma.from_documents(
            documents=all_docs,
            embedding=EmbeddingManager.get_embedder(),
            persist_directory="./chroma_db",
            collection_metadata={"hnsw:space": "cosine"}
        )


class ModelHandler:
    def __init__(self, config, trait):
        model_config = config["models"][trait]
        self.model = self._init_llm(model_config)
        self.prompt = PromptTemplate(
            template=model_config["prompt_template"],
            input_variables=["context", "question"]
        )
        self.chain = self._build_chain()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, max=10))
    def _init_llm(self, model_config):
        """åˆå§‹åŒ–å¤§æ¨¡å‹"""
        return ChatOllama(
            model=MODEL_CONFIG["llm"]["name"],
            temperature=MODEL_CONFIG["llm"]["temperature"],
            num_ctx=MODEL_CONFIG["llm"]["num_ctx"],
            base_url="http://localhost:11434",
            request_timeout=300
        )

    def _build_chain(self):
        """æ„å»ºå¤„ç†é“¾"""
        return (
                {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
                | self.prompt
                | self.model
                | StrOutputParser()
        )

    def get_response(self, message, retriever):
        """è·å–å“åº”"""
        try:
            docs = retriever.invoke(message)
            if not docs:
                return "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æˆ–ä¸Šä¼ ç›¸å…³æ–‡æ¡£", []
            context = "\n\n".join(doc.page_content for doc in docs)
            return self.chain.invoke({"context": context, "question": message}), docs
        except Exception as e:
            logger.error(f"å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise


class DocumentQAApp:
    def __init__(self):
        self.config = self._load_config()
        self.current_files = []
        self.vector_store = None
        self.index = None
        Settings.embed_model = LangchainEmbedding(EmbeddingManager.get_embedder())

    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open("config.json") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {str(e)}")
            raise

    def process_files(self, file_paths: List[str]):
        """å¤„ç†ä¸Šä¼ æ–‡ä»¶"""
        try:
            self.vector_store = DataProcessor.process_files(file_paths)
            self._build_index(file_paths)
            self.current_files = file_paths
            return f"âœ… å·²æˆåŠŸåˆ†æ {len(file_paths)} ä¸ªæ–‡æ¡£"
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}"

    def _build_index(self, file_paths: List[str]):
        """æ„å»ºç´¢å¼•"""
        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                content = []
                for fp in file_paths:
                    if fp.lower().endswith((".png", ".jpg", ".jpeg")):
                        content.append(f"# å›¾ç‰‡å†…å®¹\n\n{ImageProcessor.extract_text_from_image(fp)}")
                    else:
                        content.append(MarkdownConverter.convert_to_markdown(fp))

                tmp_path = os.path.join(tmp_dir, "combined.md")
                with open(tmp_path, "w") as f:
                    f.write("\n\n".join(content))

                self.index = VectorStoreIndex.from_documents(
                    SimpleDirectoryReader(tmp_dir).load_data()
                )
        except Exception as e:
            logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            raise

    def generate_response(self, model_trait, question):
        """ç”Ÿæˆå›ç­”"""
        if not self.vector_store or not self.index:
            return "âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶åˆ†ææ–‡æ¡£"

        try:
            retriever = self.vector_store.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": 8,
                    "filter": {"source": {"$in": self.current_files}}
                }
            )
            model_handler = ModelHandler(self.config, model_trait)
            response, _ = model_handler.get_response(question, retriever)

            retrievals = self.index.as_retriever(similarity_top_k=4).retrieve(question)
            context = "\n".join(
                f"ğŸ“Œ å‚è€ƒå†…å®¹ {i + 1}:\n{self._format_node(n)}\n"
                for i, n in enumerate(retrievals[:3])
            )

            return f"ğŸ¤– æ™ºèƒ½å›ç­”ï¼š\n{response}\n\nğŸ” å‚è€ƒä¾æ®ï¼š\n{context}"
        except Exception as e:
            logger.error(f"é—®ç­”å¤±è´¥: {str(e)}")
            return f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"

    def _format_node(self, node, length=1000):
        """æ ¼å¼åŒ–ç»“æœ"""
        content = node.node.get_content().strip()
        metadata = "\n".join(f"{k}: {v}" for k, v in node.node.metadata.items())
        return (
            f"ç›¸å…³æ€§: {node.score:.3f}\n"
            f"å…ƒæ•°æ®: {metadata}\n"
            f"å†…å®¹: {truncate_text(content, length)}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        )

    def launch(self):
        """å¯åŠ¨ç•Œé¢"""
        css = """
        .gradio-container {
            max-width: 1200px !important;
            background: #121212 !important;
        }
        .dark {background: #121212}
        .prose {
            padding: 20px; 
            background: #2D2D2D;
            border-radius: 12px; 
            box-shadow: 0 2px 6px rgba(0,0,0,0.3);
            border: 1px solid #4A1E35;
            color: #E0E0E0;
        }
        .file-upload {
            border: 2px dashed #FF69B4 !important; 
            padding: 20px;
            border-radius: 12px;
            background: #2D2D2D;
        }
        .status-box {
            background: #4A1E35; 
            padding: 15px; 
            border-radius: 8px;
            color: #FFB6C1;
        }
        .markdown h1, .markdown h2, .markdown h3 {
            color: #FF69B4 !important;
        }
        textarea, input {
            background: #2D2D2D !important;
            color: #E0E0E0 !important;
        }
        .dropdown {
            background: #2D2D2D !important;
            border-color: #4A1E35 !important;
        }
        """
        with gr.Blocks(title="Multiple2025 æ–‡æ¡£åˆ†æ", theme=custom_theme, css=css) as app:
            gr.Markdown("""
            <div style="text-align: center; margin-bottom: 30px;">
                <h1 style="color: #FF69B4; font-size: 2.5em; margin-bottom: 10px;">
                    Multiple2025 æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ
                </h1>
                <p style="color: #B0B0B0; font-size: 1.1em;">ä¸Šä¼ æ‚¨çš„æ–‡æ¡£/å›¾ç‰‡ï¼Œè·å–æ·±åº¦è¯­ä¹‰åˆ†æç»“æœ</p>
            </div>
            """)


            with gr.Row(variant="panel"):
                with gr.Column(scale=1):
                    gr.Markdown("### ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æ–‡æ¡£", elem_classes="prose")
                    with gr.Group():
                        file_input = gr.File(
                            label="æ”¯æŒæ ¼å¼ï¼šPDF/DOC/PPT/XLS/å›¾ç‰‡ç­‰",
                            file_types=SUPPORTED_EXTS,
                            type="filepath",
                            file_count="multiple",
                            elem_classes="file-upload"
                        )
                        process_btn = gr.Button("å¼€å§‹åˆ†ææ–‡æ¡£", variant="primary")

                    gr.Markdown("### åˆ†æçŠ¶æ€", elem_classes="prose")
                    status = gr.Textbox(
                        label="å¤„ç†è¿›åº¦",
                        interactive=False,
                        elem_classes="status-box"
                    )

                    gr.Markdown("### åˆ†ææ¨¡å¼", elem_classes="prose")
                    model_selector = gr.Dropdown(
                        label="é€‰æ‹©åˆ†ææ¨¡å¼",
                        choices=list(self.config["models"].keys()),
                        value="default"
                    )

                with gr.Column(scale=2):
                    gr.Markdown("### æ™ºèƒ½é—®ç­”", elem_classes="prose")
                    question_input = gr.Textbox(
                        label="è¯·è¾“å…¥é—®é¢˜",
                        placeholder="è¾“å…¥æ‚¨éœ€è¦åˆ†æçš„æ–‡æ¡£å†…å®¹ç›¸å…³é—®é¢˜...",
                        lines=4,
                        max_lines=6
                    )
                    submit_btn = gr.Button("æäº¤åˆ†æ", variant="primary")

                    gr.Markdown("### åˆ†ææŠ¥å‘Š", elem_classes="prose")
                    response_output = gr.Markdown(
                        label="åˆ†æç»“æœ",
                        elem_classes="prose",
                        show_copy_button=True
                    )

            process_btn.click(
                self.process_files,
                inputs=file_input,
                outputs=status,
                api_name="process"
            )
            submit_btn.click(
                self.generate_response,
                inputs=[model_selector, question_input],
                outputs=response_output,
                api_name="analyze"
            )

        return app


if __name__ == "__main__":
    qa_app = DocumentQAApp()
    app = qa_app.launch()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        show_api=False,
        favicon_path="./multiple2025.ico"  # è¯·æ›¿æ¢å®é™…å›¾æ ‡è·¯å¾„
    )